[Real Work Prototype Code]

Prototype Code - Version 1
Minimum Code (~100 lines)


```python
"""
Axial Topology Engine - Minimal Working Prototype
Actually runs and trains on real data
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

class AxialEngine(nn.Module):
    """
    Minimal but fully functional 3-axis engine.
    Can be trained on actual data.
    """
    
    def __init__(self, input_dim=784, hidden_dim=64, output_dim=10):
        super().__init__()
        
        # Input projection
        self.input_proj = nn.Linear(input_dim, hidden_dim)
        
        # Three axis processors
        self.why_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        self.how_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        self.what_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Output projection
        self.output_proj = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x, n_cycles=2):
        """
        Forward pass with axial circulation.
        
        Args:
            x: Input tensor (batch_size, input_dim)
            n_cycles: Number of complete Why→How→What cycles
            
        Returns:
            Output tensor (batch_size, output_dim)
        """
        # Project input to hidden space
        state = self.input_proj(x)
        
        # Circulate through axes
        for cycle in range(n_cycles):
            # Why axis: Purpose extraction
            state = self.why_processor(state) + state  # Residual
            
            # How axis: Method determination
            state = self.how_processor(state) + state  # Residual
            
            # What axis: Action generation
            state = self.what_processor(state) + state  # Residual
        
        # Project to output space
        output = self.output_proj(state)
        return output


def train_epoch(model, dataloader, optimizer, device):
    """Train for one epoch."""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_x, batch_y in dataloader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        # Forward pass
        output = model(batch_x, n_cycles=3)
        loss = F.cross_entropy(output, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Statistics
        total_loss += loss.item()
        pred = output.argmax(dim=1)
        correct += (pred == batch_y).sum().item()
        total += batch_y.size(0)
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy


def evaluate(model, dataloader, device):
    """Evaluate model."""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_x, batch_y in dataloader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            output = model(batch_x, n_cycles=3)
            pred = output.argmax(dim=1)
            correct += (pred == batch_y).sum().item()
            total += batch_y.size(0)
    
    accuracy = 100 * correct / total
    return accuracy


# ===== Usage Example =====
if __name__ == "__main__":
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Create dummy dataset (replace with real MNIST)
    # For actual use: from torchvision.datasets import MNIST
    X_train = torch.randn(1000, 784)
    y_train = torch.randint(0, 10, (1000,))
    X_test = torch.randn(200, 784)
    y_test = torch.randint(0, 10, (200,))
    
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32)
    
    # Initialize model
    model = AxialEngine(
        input_dim=784,
        hidden_dim=64,
        output_dim=10
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    print(f"\nModel parameters: {sum(p.numel() for p in model.parameters()):,}")
    print("\nStarting training...\n")
    
    # Training loop
    for epoch in range(10):
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)
        test_acc = evaluate(model, test_loader, device)
        
        print(f"Epoch {epoch+1}/10 | "
              f"Loss: {train_loss:.4f} | "
              f"Train Acc: {train_acc:.2f}% | "
              f"Test Acc: {test_acc:.2f}%")
    
    print("\n✓ Training complete!")
```


***


Prototype Code - Version 2
Including dynamic cycle (~200 lines)


```python
"""
Axial Topology Engine - Adaptive Circulation
Includes dynamic cycle adjustment based on convergence
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

class AdaptiveAxialEngine(nn.Module):
    """
    Axial engine with adaptive circulation depth.
    Easy questions converge quickly, hard ones get more cycles.
    """
    
    def __init__(
        self,
        input_dim=784,
        hidden_dim=128,
        output_dim=10,
        max_cycles=10,
        convergence_threshold=0.01
    ):
        super().__init__()
        
        self.max_cycles = max_cycles
        self.convergence_threshold = convergence_threshold
        
        # Input projection
        self.input_proj = nn.Linear(input_dim, hidden_dim)
        
        # Axis processors with layer norm
        self.why_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        self.how_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        self.what_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Output projection
        self.output_proj = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x, adaptive=True, verbose=False):
        """
        Forward pass with optional adaptive circulation.
        
        Args:
            x: Input tensor
            adaptive: If True, stop when converged
            verbose: If True, print cycle count
            
        Returns:
            output: Model output
            cycles_used: Number of cycles (if adaptive)
        """
        # Project input
        state = self.input_proj(x)
        prev_state = state.clone()
        
        cycles_used = 0
        
        for cycle in range(self.max_cycles):
            # One complete circulation
            state = self._circulate_once(state)
            cycles_used += 1
            
            # Check convergence (if adaptive)
            if adaptive:
                delta = torch.norm(state - prev_state, dim=-1).mean()
                
                if delta < self.convergence_threshold:
                    if verbose:
                        print(f"Converged in {cycles_used} cycles")
                    break
                
                prev_state = state.clone()
        
        # Project to output
        output = self.output_proj(state)
        
        if adaptive:
            return output, cycles_used
        return output
    
    def _circulate_once(self, state):
        """Perform one complete Why→How→What circulation."""
        # Why axis
        state = self.why_processor(state) + state
        
        # How axis
        state = self.how_processor(state) + state
        
        # What axis
        state = self.what_processor(state) + state
        
        return state


def demonstrate_adaptive_depth():
    """Demonstrate adaptive circulation depth."""
    print("=== Adaptive Circulation Demonstration ===\n")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AdaptiveAxialEngine(
        input_dim=784,
        hidden_dim=64,
        output_dim=10
    ).to(device)
    model.eval()
    
    # Create different "difficulty" inputs
    easy_input = torch.randn(1, 784).to(device) * 0.1  # Small values
    hard_input = torch.randn(1, 784).to(device) * 10.0  # Large values
    
    with torch.no_grad():
        # Easy input
        _, easy_cycles = model(easy_input, adaptive=True, verbose=False)
        print(f"Easy input converged in: {easy_cycles} cycles")
        
        # Hard input
        _, hard_cycles = model(hard_input, adaptive=True, verbose=False)
        print(f"Hard input converged in: {hard_cycles} cycles")
    
    print(f"\nCycle difference: {hard_cycles - easy_cycles}")


if __name__ == "__main__":
    demonstrate_adaptive_depth()
```


***


Prototype Code - Version 3
MNIST learning version (~300줄)

```python
"""
Axial Topology Engine - Complete MNIST Example
Ready to train on real MNIST dataset
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

class ProductionAxialEngine(nn.Module):
    """
    Production-ready axial engine with:
    - Batch normalization
    - Dropout
    - Skip connections
    - Configurable depth
    """
    
    def __init__(
        self,
        input_dim=784,
        hidden_dim=128,
        output_dim=10,
        dropout=0.1
    ):
        super().__init__()
        
        # Input processing
        self.input_net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Axis processors
        self.why_net = self._make_axis_processor(hidden_dim, dropout)
        self.how_net = self._make_axis_processor(hidden_dim, dropout)
        self.what_net = self._make_axis_processor(hidden_dim, dropout)
        
        # Output processing
        self.output_net = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
    def _make_axis_processor(self, hidden_dim, dropout):
        """Create an axis processor module."""
        return nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim)
        )
    
    def forward(self, x, n_cycles=3):
        """
        Forward pass.
        
        Args:
            x: Input (batch_size, input_dim) or (batch_size, 1, 28, 28)
            n_cycles: Number of circulations
        """
        # Flatten if needed
        if x.dim() == 4:
            x = x.view(x.size(0), -1)
        
        # Input processing
        state = self.input_net(x)
        
        # Circulate through axes
        for _ in range(n_cycles):
            # Why → How → What with residual connections
            state = state + self.why_net(state)
            state = state + self.how_net(state)
            state = state + self.what_net(state)
        
        # Output
        return self.output_net(state)


class Trainer:
    """Training manager."""
    
    def __init__(self, model, device, lr=0.001):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', patience=2, factor=0.5
        )
        
        self.train_losses = []
        self.train_accs = []
        self.test_accs = []
    
    def train_epoch(self, train_loader):
        """Train for one epoch."""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(self.device)
            batch_y = batch_y.to(self.device)
            
            # Forward
            output = self.model(batch_x, n_cycles=3)
            loss = F.cross_entropy(output, batch_y)
            
            # Backward
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # Stats
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += (pred == batch_y).sum().item()
            total += batch_y.size(0)
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100 * correct / total
        
        return avg_loss, accuracy
    
    def evaluate(self, test_loader):
        """Evaluate on test set."""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                output = self.model(batch_x, n_cycles=3)
                pred = output.argmax(dim=1)
                correct += (pred == batch_y).sum().item()
                total += batch_y.size(0)
        
        accuracy = 100 * correct / total
        return accuracy
    
    def train(self, train_loader, test_loader, epochs=10):
        """Complete training loop."""
        print("Starting training...\n")
        
        for epoch in range(epochs):
            train_loss, train_acc = self.train_epoch(train_loader)
            test_acc = self.evaluate(test_loader)
            
            # Update learning rate
            self.scheduler.step(test_acc)
            
            # Record metrics
            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)
            self.test_accs.append(test_acc)
            
            print(f"Epoch {epoch+1:2d}/{epochs} | "
                  f"Loss: {train_loss:.4f} | "
                  f"Train: {train_acc:.2f}% | "
                  f"Test: {test_acc:.2f}%")
        
        print("\n✓ Training complete!")
        return self.train_losses, self.train_accs, self.test_accs
    
    def plot_results(self):
        """Plot training results."""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # Loss
        ax1.plot(self.train_losses)
        ax1.set_title('Training Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True)
        
        # Accuracy
        ax2.plot(self.train_accs, label='Train')
        ax2.plot(self.test_accs, label='Test')
        ax2.set_title('Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_results.png')
        print("Results saved to training_results.png")


def main():
    """Main training script."""
    
    # Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}\n")
    
    # Data loading
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST(
        './data', train=True, download=True, transform=transform
    )
    test_dataset = datasets.MNIST(
        './data', train=False, transform=transform
    )
    
    train_loader = DataLoader(
        train_dataset, batch_size=128, shuffle=True, num_workers=2
    )
    test_loader = DataLoader(
        test_dataset, batch_size=128, shuffle=False, num_workers=2
    )
    
    # Model
    model = ProductionAxialEngine(
        input_dim=784,
        hidden_dim=128,
        output_dim=10,
        dropout=0.2
    )
    
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}\n")
    
    # Training
    trainer = Trainer(model, device, lr=0.001)
    trainer.train(train_loader, test_loader, epochs=10)
    
    # Plot results
    try:
        trainer.plot_results()
    except:
        print("(Plotting skipped - matplotlib not in display mode)")


if __name__ == "__main__":
    main()
```
